{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet google-cloud-documentai==2.31.0\n",
        "\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    print(\"Authenticated\")\n",
        "\n",
        "PROJECT_ID = \"<PROJECT_NAME_FROM_SETUP_NOTEBOOK>\"\n",
        "PROJECT_NUMBER = \"<PROJECT_NUMBER_FROM_SETUP_NOTEBOOK>\"\n",
        "DATASET_ID = \"<DATASET_ID_FROM_SETUP_NOTEBOOK>\"\n",
        "\n",
        "CONNECTION_NAME = \"<CONNECTION_NAME_FROM_SETUP_NOTEBOOK>\"\n",
        "BUCKET_NAME = \"<BUCKET_NAME_FROM_SETUP_NOTEBOOK>\"\n",
        "from google.cloud import bigquery, storage\n",
        "\n",
        "\n",
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "# Get the list of existing tables in the dataset\n",
        "existing_tables = [\n",
        "    table.table_id for table in bq_client.list_tables(DATASET_ID)\n",
        "]\n",
        "\n",
        "# Get the list of PDF files in the GCS bucket\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "blobs = bucket.list_blobs()\n",
        "\n",
        "for blob in blobs:\n",
        "    if blob.name.endswith(\".pdf\"):\n",
        "        table_name = blob.name.rsplit(\".\", 1)[0]  # Extract table name from filename (up to .pdf)\n",
        "        if table_name not in existing_tables:\n",
        "            # Create external table for this PDF\n",
        "            query = f\"\"\"\n",
        "            CREATE OR REPLACE EXTERNAL TABLE `{DATASET_ID}.{table_name}`\n",
        "            WITH CONNECTION `{CONNECTION_NAME}`\n",
        "            OPTIONS (\n",
        "              uris = ['gs://{BUCKET_NAME}/{blob.name}'],\n",
        "              object_metadata = 'DIRECTORY'\n",
        "            );\n",
        "            \"\"\"\n",
        "\n",
        "            query_job = bq_client.query(query)  # API request\n",
        "            query_job.result()  # Waits for the query to complete\n",
        "\n",
        "            print(f\"External table {DATASET_ID}.{table_name} created successfully.\")\n",
        "        else:\n",
        "            print(f\"Table {DATASET_ID}.{table_name} already exists. Skipping.\")\n",
        "\n",
        "from google.cloud import bigquery, storage\n",
        "import re\n",
        "\n",
        "# Get the list of existing tables in the dataset\n",
        "existing_tables = [\n",
        "    table.table_id for table in bq_client.list_tables(DATASET_ID)\n",
        "]\n",
        "\n",
        "# Filter numeric-only table names\n",
        "numeric_tables = [\n",
        "    table for table in existing_tables if re.match(r'^\\d+$', table)\n",
        "]\n",
        "\n",
        "# Iterate through numeric-only tables and create _result and _result_parsed tables\n",
        "for table_name in numeric_tables:\n",
        "    # Create _result table\n",
        "    result_table_name = f\"{table_name}_result\"\n",
        "    result_query = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE {DATASET_ID}.{result_table_name} AS (\n",
        "      SELECT * FROM ML.PROCESS_DOCUMENT(\n",
        "        MODEL {DATASET_ID}.layout_parser,\n",
        "        TABLE {DATASET_ID}.{table_name},\n",
        "        PROCESS_OPTIONS => (JSON '{{\"layout_config\": {{\"chunking_config\": {{\"chunk_size\": 250}}}}}}')\n",
        "      )\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    result_query_job = bq_client.query(result_query)\n",
        "    result_query_job.result()\n",
        "    print(f\"Table {result_table_name} created successfully.\")\n",
        "\n",
        "    # Create _result_parsed table\n",
        "    result_parsed_table_name = f\"{table_name}_result_parsed\"\n",
        "    result_parsed_query = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE {DATASET_ID}.{result_parsed_table_name} AS (\n",
        "      SELECT\n",
        "        uri,\n",
        "        JSON_EXTRACT_SCALAR(json , '$.chunkId') AS id,\n",
        "        JSON_EXTRACT_SCALAR(json , '$.content') AS content,\n",
        "        JSON_EXTRACT_SCALAR(json , '$.pageFooters[0].text') AS page_footers_text,\n",
        "        JSON_EXTRACT_SCALAR(json , '$.pageSpan.pageStart') AS page_span_start,\n",
        "        JSON_EXTRACT_SCALAR(json , '$.pageSpan.pageEnd') AS page_span_end\n",
        "      FROM {DATASET_ID}.{result_table_name}, UNNEST(JSON_EXTRACT_ARRAY(ml_process_document_result.chunkedDocument.chunks, '$')) json\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    result_parsed_query_job = bq_client.query(result_parsed_query)\n",
        "    result_parsed_query_job.result()\n",
        "    print(f\"Table {result_parsed_table_name} created successfully.\")\n",
        "\n",
        "\n",
        "# Initialize BigQuery client\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Get the list of existing tables in the dataset\n",
        "existing_tables = [\n",
        "    table.table_id for table in bq_client.list_tables(DATASET_ID)\n",
        "]\n",
        "\n",
        "# Filter numeric-only table names excluding _result and _result_parsed\n",
        "numeric_tables = [\n",
        "    table for table in existing_tables if re.match(r'^\\d+$', table)\n",
        "]\n",
        "\n",
        "# Iterate through numeric-only tables and create embeddings table\n",
        "for table_name in numeric_tables:\n",
        "    embeddings_table_name = f\"{table_name}_embeddings\"\n",
        "    result_parsed_table_name = f\"{table_name}_result_parsed\"\n",
        "\n",
        "    if result_parsed_table_name in existing_tables:\n",
        "        embeddings_query = f\"\"\"\n",
        "        CREATE OR REPLACE TABLE {DATASET_ID}.{embeddings_table_name} AS\n",
        "        SELECT * FROM ML.GENERATE_EMBEDDING(\n",
        "          MODEL {DATASET_ID}.embedding_model,\n",
        "          TABLE {DATASET_ID}.{result_parsed_table_name}\n",
        "        );\n",
        "        \"\"\"\n",
        "\n",
        "        embeddings_query_job = bq_client.query(embeddings_query)\n",
        "        embeddings_query_job.result()\n",
        "        print(f\"Table {embeddings_table_name} created successfully.\")\n",
        "    else:\n",
        "        print(f\"Required table {result_parsed_table_name} does not exist. Skipping embedding table creation for {table_name}.\")\n",
        "\n",
        "def get_deal_ids():\n",
        "    \"\"\"Fetch deal_id values from the sows CTE.\"\"\"\n",
        "    client = bigquery.Client()\n",
        "\n",
        "    sows_query = \"\"\"\n",
        "    SELECT SAFE_CAST(table_name AS INT64) AS deal_id\n",
        "    FROM {DATASET_ID}.INFORMATION_SCHEMA.TABLES\n",
        "    WHERE SAFE_CAST(table_name AS INT64) IS NOT NULL\n",
        "    AND SAFE_CAST(table_name AS INT64) not in (SELECT deal_id from {DATASET_ID}.engagement_details )\n",
        "    \"\"\"\n",
        "\n",
        "    deal_ids = [row[\"deal_id\"] for row in client.query(sows_query)]\n",
        "    return deal_ids\n",
        "\n",
        "def construct_query(deal_ids):\n",
        "    \"\"\"Construct the full SQL query dynamically for all deal_id values.\"\"\"\n",
        "    query_parts = []\n",
        "\n",
        "    for deal_id in deal_ids:\n",
        "        query_parts.append(f\"\"\"\n",
        "        select * from (with objectives as (\n",
        "        SELECT\n",
        "          {deal_id} AS deal_id,\n",
        "          ml_generate_text_llm_result AS objectives_json,\n",
        "          NULL AS engagement_summary,\n",
        "          NULL AS engagement_deliverables\n",
        "        FROM\n",
        "          ML.GENERATE_TEXT(\n",
        "            MODEL `{DATASET_ID}.gemini_flash`,\n",
        "            (\n",
        "              SELECT\n",
        "                CONCAT(\n",
        "                  'What are the (up to 10) objectives, expected business benefits and measurement metrics for this project? Be concise, add no preamble and respond in JSON format using output fields \"objective_number\", \"objective_details\", \"expected_benefit\",\"objective_metric\" using the following context:',\n",
        "                  STRING_AGG(FORMAT(\"context: %s and reference: %s\", base.content, base.uri), ',')) AS prompt\n",
        "              FROM VECTOR_SEARCH(\n",
        "                TABLE `{DATASET_ID}.{deal_id}_embeddings`,\n",
        "                'ml_generate_embedding_result',\n",
        "                (\n",
        "                  SELECT\n",
        "                    ml_generate_embedding_result,\n",
        "                    content AS query\n",
        "                  FROM\n",
        "                    ML.GENERATE_EMBEDDING(\n",
        "                      MODEL `{DATASET_ID}.embedding_model`,\n",
        "                      (\n",
        "                        SELECT\n",
        "                          'What are the objectives of this project? Be concise and use the following context' AS content\n",
        "                      )\n",
        "                    )\n",
        "                ),\n",
        "                TOP_K => 10,\n",
        "                OPTIONS => '{{\"fraction_lists_to_search\": 0.01}}'\n",
        "              )\n",
        "            ),\n",
        "            STRUCT(2048 AS max_output_tokens, TRUE AS flatten_json_output)\n",
        "          )),\n",
        "        engagement_summary as (\n",
        "        -- Engagement summary for deal_id {deal_id}\n",
        "        SELECT\n",
        "          {deal_id} AS deal_id,\n",
        "          NULL AS objectives_json,\n",
        "          ml_generate_text_llm_result AS engagement_summary,\n",
        "          NULL AS engagement_deliverables\n",
        "        FROM\n",
        "          ML.GENERATE_TEXT(\n",
        "            MODEL `{DATASET_ID}.gemini_flash`,\n",
        "            (\n",
        "              SELECT\n",
        "                CONCAT(\n",
        "                  'Summarise the background, business requirements, solution and assumptions for this project. Be concise and respond in JSON format using output fields \"Background\", \"Requirements\",\"Solution\", \"Assumptions\", add no preamble and be factual:',\n",
        "                  STRING_AGG(FORMAT(\"context: %s and reference: %s\", base.content, base.uri), ',')) AS prompt,\n",
        "              FROM VECTOR_SEARCH(\n",
        "                TABLE `{DATASET_ID}.{deal_id}_embeddings`,\n",
        "                'ml_generate_embedding_result',\n",
        "                (\n",
        "                  SELECT\n",
        "                    ml_generate_embedding_result,\n",
        "                    content AS query\n",
        "                  FROM\n",
        "                    ML.GENERATE_EMBEDDING(\n",
        "                      MODEL `{DATASET_ID}.embedding_model`,\n",
        "                      (\n",
        "                        SELECT\n",
        "                          'Summarise the background, requirements and solution for this project' AS content\n",
        "                      )\n",
        "                    )\n",
        "                ),\n",
        "                TOP_K => 10,\n",
        "                OPTIONS => '{{\"fraction_lists_to_search\": 0.01}}'\n",
        "              )\n",
        "            ),\n",
        "            STRUCT(2048 AS max_output_tokens, TRUE AS flatten_json_output)\n",
        "          )),\n",
        "\n",
        "        engagement_deliverables as (\n",
        "        SELECT\n",
        "          {deal_id} AS deal_id,\n",
        "          NULL AS objectives_json,\n",
        "          NULL AS engagement_summary,\n",
        "          ml_generate_text_llm_result AS engagement_deliverables\n",
        "        FROM\n",
        "          ML.GENERATE_TEXT(\n",
        "            MODEL `{DATASET_ID}.gemini_flash`,\n",
        "            (\n",
        "              SELECT\n",
        "                CONCAT(\n",
        "                  'What are the (up to 10) contractual deliverables listed for this project? Be concise, add no preamble and respond in JSON format using output fields \"deliverable_number\", \"deliverable_details\", \"deliverable_format\", \"acceptance_criteria\" using the following context:',\n",
        "                  STRING_AGG(FORMAT(\"context: %s and reference: %s\", base.content, base.uri), ',')) AS prompt\n",
        "              FROM VECTOR_SEARCH(\n",
        "                TABLE `{DATASET_ID}.{deal_id}_embeddings`,\n",
        "                'ml_generate_embedding_result',\n",
        "                (\n",
        "                  SELECT\n",
        "                    ml_generate_embedding_result,\n",
        "                    content AS query\n",
        "                  FROM\n",
        "                    ML.GENERATE_EMBEDDING(\n",
        "                      MODEL `{DATASET_ID}.embedding_model`,\n",
        "                      (\n",
        "                        SELECT\n",
        "                          'What are the (up to 10) contractual deliverables listed for this project? Be concise and use the following context' AS content\n",
        "                      )\n",
        "                    )\n",
        "                ),\n",
        "                TOP_K => 10,\n",
        "                OPTIONS => '{{\"fraction_lists_to_search\": 0.01}}'\n",
        "              )\n",
        "            ),\n",
        "            STRUCT(2048 AS max_output_tokens, TRUE AS flatten_json_output)\n",
        "          ))\n",
        "        SELECT\n",
        "        o.deal_id,\n",
        "        JSON_VALUE(REPLACE(REPLACE(s.engagement_summary,'```json',''),'```',''), \"$.Background\") AS background,\n",
        "        JSON_VALUE(REPLACE(REPLACE(s.engagement_summary,'```json',''),'```',''), \"$.Requirements\") AS requirements,\n",
        "        JSON_VALUE(REPLACE(REPLACE(s.engagement_summary,'```json',''),'```',''), \"$.Solution\") AS solution,\n",
        "        ARRAY(\n",
        "          SELECT AS STRUCT\n",
        "            JSON_EXTRACT_SCALAR(item, '$.objective_number') AS objective_number,\n",
        "            JSON_EXTRACT_SCALAR(item, '$.objective_details') AS objective_details,\n",
        "            JSON_EXTRACT_SCALAR(item, '$.expected_benefit') AS expected_benefit,\n",
        "            JSON_EXTRACT_SCALAR(item, '$.objective_metric') AS objective_metric\n",
        "          FROM UNNEST(JSON_EXTRACT_ARRAY(REPLACE(REPLACE(o.objectives_json,'```json',''),'```',''))) AS item\n",
        "        ) AS objectives,\n",
        "        ARRAY(\n",
        "          SELECT AS STRUCT\n",
        "            JSON_EXTRACT_SCALAR(item, '$.deliverable_number') AS deliverable_number,\n",
        "            JSON_EXTRACT_SCALAR(item, '$.deliverable_details') AS deliverable_details,\n",
        "            JSON_EXTRACT_SCALAR(item, '$.deliverable_format') AS deliverable_format,\n",
        "            JSON_EXTRACT_SCALAR(item, '$.acceptance_criteria') AS acceptance_criteria\n",
        "          FROM UNNEST(JSON_EXTRACT_ARRAY(REPLACE(REPLACE(d.engagement_deliverables,'```json',''),'```',''))) AS item\n",
        "        ) AS deliverables,\n",
        "        REPLACE(REPLACE(o.objectives_json,'```json',''),'```','') as objectives_json,\n",
        "        REPLACE(REPLACE(d.engagement_deliverables,'```json',''),'```','') as deliverables_json\n",
        "      FROM\n",
        "      objectives o\n",
        "      LEFT JOIN engagement_summary s ON o.deal_id = s.deal_id\n",
        "      LEFT JOIN engagement_deliverables d ON o.deal_id = d.deal_id)\n",
        "        \"\"\")\n",
        "    return \" UNION ALL \".join(query_parts)\n",
        "\n",
        "def replicate_eu_to_europe_west2():\n",
        "\n",
        "    from google.cloud import bigquery\n",
        "    from google.cloud import storage\n",
        "\n",
        "    # Initialize BigQuery and GCS clients\n",
        "    bq_client = bigquery.Client()\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Variables\n",
        "    source_project = \"ra-development\"\n",
        "    source_dataset = \"{DATASET_ID}\"\n",
        "    source_table = \"engagement_details\"\n",
        "    source_region = \"EU\"\n",
        "\n",
        "    target_project = \"ra-development\"\n",
        "    target_dataset = \"{DATASET_ID}_europe_west2\"\n",
        "    target_table = \"engagement_details\"\n",
        "    target_region = \"europe-west2\"\n",
        "\n",
        "    gcs_bucket_name = \"<BUCKET_NAME_FROM_PREVIOUS_NOTEBOOK>\"  # Ensure this bucket is in the `EU` region\n",
        "    export_uri = f\"gs://{gcs_bucket_name}/{source_table}.avro\"  # Using Avro for structured data\n",
        "\n",
        "    # Step 1: Export the source table to GCS in Avro format\n",
        "    extract_job = bq_client.extract_table(\n",
        "        f\"{source_project}.{source_dataset}.{source_table}\",\n",
        "        export_uri,\n",
        "        job_config=bigquery.job.ExtractJobConfig(destination_format=\"AVRO\"),\n",
        "        location=source_region  # Ensure the export happens in the source table's region\n",
        "    )\n",
        "    extract_job.result()  # Wait for the job to complete\n",
        "    print(f\"Exported {source_table} to {export_uri}\")\n",
        "\n",
        "    # Step 2: Load the exported data from GCS into the target table\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.AVRO,  # Avro preserves nested STRUCTs\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE  # Overwrite the table if it exists\n",
        "    )\n",
        "\n",
        "    load_job = bq_client.load_table_from_uri(\n",
        "        export_uri,\n",
        "        f\"{target_project}.{target_dataset}.{target_table}\",\n",
        "        job_config=job_config,\n",
        "        location=target_region  # Ensure the load happens in the target dataset's region\n",
        "    )\n",
        "    load_job.result()  # Wait for the job to complete\n",
        "    print(f\"Loaded data into {target_dataset}.{target_table} in region {target_region}\")\n",
        "\n",
        "    # Step 3: Cleanup GCS (optional)\n",
        "    bucket = storage_client.bucket(gcs_bucket_name)\n",
        "    blob = bucket.blob(f\"{source_table}.avro\")\n",
        "    blob.delete()\n",
        "    print(f\"Deleted temporary file {export_uri} from GCS\")\n",
        "\n",
        "def execute_query(query):\n",
        "    \"\"\"Execute the constructed SQL query.\"\"\"\n",
        "    client = bigquery.Client()\n",
        "    query_job = client.query(\"INSERT INTO {DATASET_ID}.engagement_details (deal_id, background, requirements, solution, objectives, deliverables, objectives_json, deliverables_json)  \"+query)\n",
        "    return query_job.result()\n",
        "\n",
        "def main():\n",
        "    deal_ids = get_deal_ids()\n",
        "    if not deal_ids:\n",
        "        replicate_eu_to_europe_west2()\n",
        "        print(\"No deal_ids found in sows.\")\n",
        "        return\n",
        "\n",
        "    full_query = construct_query(deal_ids)\n",
        "    results = execute_query(full_query)\n",
        "\n",
        "    replicate_eu_to_europe_west2()\n",
        "\n",
        "    for row in results:\n",
        "        print(row)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "0B6hOfYXlHhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}